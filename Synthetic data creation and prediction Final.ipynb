{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd744bf-7add-4c12-b497-d906d917a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"society_ranges = {\n",
    "    \"Society_1\": {\"qty\": (150, 200), \"fat\": (3.9, 4.1), \"snf\": (7.9, 8.2)},\n",
    "    \"Society_2\": {\"qty\": (100, 130), \"fat\": (3.6, 3.9), \"snf\": (7.7, 7.9)},\n",
    "    \"Society_3\": {\"qty\": (80, 110), \"fat\": (3.5, 3.8), \"snf\": (7.6, 7.8)},\n",
    "    \"Society_4\": {\"qty\": (120, 150), \"fat\": (3.7, 4.0), \"snf\": (7.8, 8.0)},\n",
    "    \"Society_5\": {\"qty\": (90, 140), \"fat\": (3.5, 3.9), \"snf\": (7.7, 7.9)}\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "747244c4-b3b9-45b1-9931-57769abf345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation_train_model(society_ranges,days):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    society=list(society_ranges.keys())\n",
    "    df=pd.DataFrame({'Date':pd.date_range(start='2025-08-01',periods=days,freq='D')})\n",
    "    #qty=np.random.randint(r[0],r[1],60)\n",
    "    import random\n",
    "    random.seed(42)\n",
    "\n",
    "    for name,r in society_ranges.items():\n",
    "        qty=np.random.randint(r['qty'][0],r['qty'][1],days)\n",
    "        fat=np.round(np.random.uniform(r['fat'][0],r['fat'][1],days),1)\n",
    "        snf=np.round(np.random.uniform(r['snf'][0],r['snf'][1],days),1)\n",
    "        ts=np.round((fat+snf),1)\n",
    "        weighted_fat=np.round(qty*fat/100,3)\n",
    "        weighted_snf=np.round(qty*snf/100,3)\n",
    "        weighted_average=np.round((qty*(fat+snf)/100),3)\n",
    "        df[f\"{name}_QTY\"]=qty\n",
    "        df[f\"{name}_Fat\"]=fat\n",
    "        df[f\"{name}_Snf\"]=snf\n",
    "        df[f\"{name}_TS\"]=ts\n",
    "        df[f\"{name}_Weighted_Fat\"]=weighted_fat\n",
    "        df[f\"{name}_Weighted_Snf\"]= weighted_snf\n",
    "        df[f\"{name}_Weighted_Average\"]=weighted_average\n",
    "    df['Overall_Weighted_Average']=df[[f'{s}_Weighted_Average' for s in society]].sum(axis=1)\n",
    "    df['Overall_Weighted_Fat']=df[[f\"{s}_Weighted_Fat\" for s in society]].sum(axis=1)\n",
    "    df['Overall_Weighted_Snf']=df[[f\"{s}_Weighted_Snf\" for s in society]].sum(axis=1)\n",
    "    df['Total_Qty']=df[[f\"{s}_QTY\" for s in society]].sum(axis=1)\n",
    "    df['Overall_Fat'] = np.round((df['Overall_Weighted_Fat'] * 100) / df['Total_Qty'], 1)\n",
    "    df['Overall_SNF'] = np.round((df['Overall_Weighted_Snf'] * 100) / df['Total_Qty'], 1)\n",
    "    df['Overall_TS']=np.round((df['Overall_Fat']+df['Overall_SNF']),1)\n",
    "    \n",
    "    #print(df.head(10))\n",
    "\n",
    "    qty_col=[f\"{s}_QTY\" for s in society]\n",
    "\n",
    "    x_fat=df[qty_col+['Overall_Weighted_Fat','Overall_Fat']]\n",
    "    y_fat=df[[f\"{s}_Weighted_Fat\" for s in society]]\n",
    "\n",
    "    x_Snf=df[qty_col+['Overall_Weighted_Snf','Overall_SNF']]\n",
    "    y_Snf=df[[f\"{s}_Weighted_Snf\" for s in society]]\n",
    "\n",
    "    x_WA=df[qty_col+['Overall_Weighted_Average','Overall_TS']]\n",
    "    y_WA=df[[f\"{s}_Weighted_Average\" for s in society]]\n",
    "\n",
    "    def split(x,y):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        return train_test_split(x,y,test_size=1/3,random_state=0)\n",
    "\n",
    "    x_train_f,x_test_f,y_train_f,y_test_f=split(x_fat,y_fat)\n",
    "    x_train_snf,x_test_snf,y_train_snf,y_test_snf=split(x_Snf,y_Snf)\n",
    "    x_train_wa,x_test_wa,y_train_wa,y_test_wa=split(x_WA,y_WA)\n",
    "\n",
    "    def scale(x_train,x_test,y_train,y_test):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        xscaler=StandardScaler()\n",
    "        yscaler=StandardScaler()\n",
    "        x_train_sc=xscaler.fit_transform(x_train)\n",
    "        x_test_sc=xscaler.transform(x_test)\n",
    "        y_train_sc=yscaler.fit_transform(y_train)\n",
    "        y_test_sc=yscaler.transform(y_test)\n",
    "        return x_train_sc,x_test_sc,y_train_sc,y_test_sc,xscaler,yscaler\n",
    "\n",
    "    x_train_f_sc,x_test_f_sc,y_train_f_sc,y_test_f_sc,xscaler_fat,yscaler_fat=scale(x_train_f,x_test_f,y_train_f,y_test_f)\n",
    "    x_train_snf_sc,x_test_snf_sc,y_train_snf_sc,y_test_snf_sc,xscaler_snf,yscaler_snf=scale(x_train_snf,x_test_snf,y_train_snf,y_test_snf)\n",
    "    x_train_wa_sc,x_test_wa_sc,y_train_wa_sc,y_test_wa_sc,xscaler_WA,yscaler_Wa=scale(x_train_wa,x_test_wa,y_train_wa,y_test_wa)\n",
    "\n",
    "    import joblib\n",
    "    fat_in_sc=\"Synthetic_Fat_input_Scaler.joblib\"\n",
    "    fat_out_sc=\"Synthetic_Fat_output_Scaler.joblib\"\n",
    "    snf_in_sc=\"Synthetic_Snf_input_Scaler.joblib\"\n",
    "    snf_out_sc=\"Synthetic_Snf_output_Scaler.joblib\"\n",
    "    wa_in_sc=\"Synthetic_WA_input_Scaler.joblib\"\n",
    "    wa_out_sc=\"Synthetic_WA_output_Scaler.joblib\"\n",
    "    joblib.dump(xscaler_fat,fat_in_sc)\n",
    "    joblib.dump(yscaler_fat,fat_out_sc)\n",
    "    joblib.dump(xscaler_snf,snf_in_sc)\n",
    "    joblib.dump(yscaler_snf,snf_out_sc)\n",
    "    joblib.dump(xscaler_WA,wa_in_sc)\n",
    "    joblib.dump(xscaler_WA,wa_out_sc)\n",
    "\n",
    "    def model_evaluation(x_train_sc,x_test_sc,y_train_sc,y_test_sc,name):\n",
    "        from sklearn.linear_model import RidgeCV\n",
    "        from sklearn.multioutput import MultiOutputRegressor as MOR\n",
    "        from sklearn.metrics import r2_score, mean_absolute_error\n",
    "        alpha=[0.1,1.0,10.0,100.0]\n",
    "        model=MOR(RidgeCV(alphas=alpha,cv=5,scoring='neg_mean_squared_error'))\n",
    "        model.fit(x_train_sc,y_train_sc)\n",
    "        y_pred=model.predict(x_test_sc)\n",
    "        r_score=r2_score(y_test_sc,y_pred)\n",
    "        mae=mean_absolute_error(y_test_sc,y_pred)\n",
    "        print(f\"{name} Model → R²: {r_score:.4f}, MAE: {mae:.4f}\")\n",
    "        return model,r_score,mae\n",
    "\n",
    "    fat_model,r_score_f,mae_f=model_evaluation(x_train_f_sc,x_test_f_sc,y_train_f_sc,y_test_f_sc,\"Fat\")\n",
    "    snf_model,r_score_snf,mae_snf=model_evaluation(x_train_snf_sc,x_test_snf_sc,y_train_snf_sc,y_test_snf_sc,\"SNF\")\n",
    "    Weighted_average_model,r_score_wa,mae_wa=model_evaluation(x_train_wa_sc,x_test_wa_sc,y_train_wa_sc,y_test_wa_sc,\"Weighted Average\")\n",
    "\n",
    "    fat_pred=\"Fat_Prediction_synthetic.joblib\"\n",
    "    snf_pred=\"Snf_Prediction_synthetic.joblib\"\n",
    "    wa_pred=\"Weighted_Average_Prediction_synthetic.joblib\"\n",
    "    joblib.dump(fat_model,fat_pred)\n",
    "    joblib.dump(snf_model,snf_pred)\n",
    "    joblib.dump(Weighted_average_model,wa_pred)\n",
    "    \n",
    "\n",
    "    \n",
    "    return {\"data\": df,\"metrics\": {\"Fat\": {\"R2\": r_score_f, \"MAE\": mae_f},\"SNF\": {\"R2\": r_score_snf, \"MAE\": mae_snf},\"Weighted_Avg\": {\"R2\": r_score_wa, \"MAE\": mae_wa}}}\n",
    "    \n",
    "   \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13977f59-657c-4b19-8324-834ddc9a22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "society_ranges = {\n",
    "    \"Society_1\": {\"qty\": (150, 200), \"fat\": (3.9, 4.1), \"snf\": (7.9, 8.2)},\n",
    "    \"Society_2\": {\"qty\": (100, 130), \"fat\": (3.6, 3.9), \"snf\": (7.7, 7.9)},\n",
    "    \"Society_3\": {\"qty\": (80, 110), \"fat\": (3.5, 3.8), \"snf\": (7.6, 7.8)},\n",
    "    \"Society_4\": {\"qty\": (120, 150), \"fat\": (3.7, 4.0), \"snf\": (7.8, 8.0)},\n",
    "    \"Society_5\": {\"qty\": (90, 140), \"fat\": (3.5, 3.9), \"snf\": (7.7, 7.9)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0539e046-63a9-4eb8-ab89-9bae20aae7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fat Model → R²: 0.9071, MAE: 0.2324\n",
      "SNF Model → R²: 0.9877, MAE: 0.0794\n",
      "Weighted Average Model → R²: 0.9809, MAE: 0.1031\n"
     ]
    }
   ],
   "source": [
    "data,metrics=creation_train_model(society_ranges,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985586fe-9698-4279-8784-046352f9337e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daac868-fcd9-4c1f-bfc8-ac3589c37ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1cb63-5b04-4793-9eb1-b0b6da05267f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f1b8e6-cb50-4f1f-8100-aa45a8b53720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb2388-13d8-4099-8298-b76cc7c2738d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
